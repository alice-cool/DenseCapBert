# DenseCapBert
Modern VQA models are easily affected by language priors, which ignore image information and learn the superficial relationship between questions
and answers, even in the optimal pre-training model. 
The main reason is that visual information is not fully extracted and
utilized, which results in a domain gap between vision and
language modalities to a certain extent. 
We propose to extract dense captions (auxiliary semantic information) from images to enhance the visual
information for reasoning and utilize them to release the gap
between vision and language.
